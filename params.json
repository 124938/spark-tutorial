{"name":"spark-tutorial","tagline":"This tutorial provides a quick introduction to using Spark","body":"```\r\n                               _               _             _                    _           _ \r\n                              | |             | |           | |                  (_)         | |\r\n  ___   _ __     __ _   _ __  | | __  ______  | |_   _   _  | |_    ___    _ __   _    __ _  | |\r\n / __| | '_ \\   / _` | | '__| | |/ / |______| | __| | | | | | __|  / _ \\  | '__| | |  / _` | | |\r\n \\__ \\ | |_) | | (_| | | |    |   <           | |_  | |_| | | |_  | (_) | | |    | | | (_| | | |\r\n |___/ | .__/   \\__,_| |_|    |_|\\_\\           \\__|  \\__,_|  \\__|  \\___/  |_|    |_|  \\__,_| |_|\r\n       | |                                                                                      \r\n       |_|                                                                                      \r\n```\r\n\r\nThis tutorial provides a quick introduction to using Spark. It demonstrates the basic functionality of RDD and DataFrame API\r\n\r\n#### Initializing Spark\r\n\r\n```scala\r\nval conf = new SparkConf().setAppName(appName).setMaster(master)\r\nnew SparkContext(conf)\r\n```\r\n\r\nCheck [SparkCommon](src/main/scala/com/tutorial/utils/SparkCommon.scala)\r\n\r\n`Note:` Only one SparkContext may be active per JVM. You must stop() the active SparkContext before creating a new one.\r\n\r\nWe have tried to cover basics of Spark Core, SQL, Streaming, ML and GraphX programming contexts.\r\n\r\n#### RDD Functionality\r\n* Creations\r\n* Transformations\r\n* Actions\r\n* Operations\r\n\r\nFor more information [check](https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-Core)\r\n\r\n\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}